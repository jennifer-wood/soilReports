---
title: null
output:
  html_document:
    mathjax: null
    jquery: null
    smart: no
    keep_md: no
---

```{r report-metadata, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
## version number
.report.version <- '1.8'

## short description
.report.description <- 'compare stack of raster data, sampled from polygons associated with 1-8 map units'
```

```{r setup, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# setup
library(knitr, quietly=TRUE)

# package options
opts_knit$set(message=FALSE, warning=FALSE, verbose=FALSE, progress=FALSE)

# chunk options
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.align='center', fig.retina=2, dev='png', antialias='cleartype', tidy=FALSE)

# R session options
options(width=100, stringsAsFactors=FALSE)


## custom functions

# return DF with polygon IDs for those polygons that have p.crit % of samples outside of 5-95th pctiles
flagPolygons <- function(i, p.crit=0.15) {
  
   # convert to values -> quantiles
  e.i <- ecdf(i$value)
  q.i <- e.i(i$value)
  # locate those samples outside of our 5th-95th range
  out.idx <- which(q.i < 0.05 | q.i > 0.95)
  
  ## TODO: may need to protect against no matching rows?
  tab <- sort(prop.table(table(i$pID[out.idx])), decreasing = TRUE)
  df <- data.frame(pID=names(tab), prop.outside.range=round(as.vector(tab), 2))
  # keep only those with > 15% of samples outside of range
  df <- df[which(df$prop.outside.range > p.crit), ] 

  return(df)
}


# http://stackoverflow.com/questions/16225530/contours-of-percentiles-on-level-plot
kdeContours <- function(i, prob) {
  this.id <- unique(i$.id)
  this.col <- cols[match(this.id, mu.set)]
  dens <- kde2d(i$x, i$y, n=200); ## estimate the z counts

  dx <- diff(dens$x[1:2])
  dy <- diff(dens$y[1:2])
  sz <- sort(dens$z)
  c1 <- cumsum(sz) * dx * dy
  levels <- sapply(prob, function(x) {
    approx(c1, sz, xout = 1 - x)$y
  })
  
  # add contours if possibly
  if(!is.na(levels))
    contour(dens, levels=levels, drawlabels=FALSE, add=TRUE, col=this.col, lwd=2)
  
  # # add bivariate medians
  # points(median(i$x), median(i$y), pch=3, lwd=2, col=this.col)
}


# masking function applied to a "wide" data.frame of sampled raster data
# function is applied column-wise
# note: using \leq and \geq for cases with very narrow distributions
mask.fun <- function(i) {
  res <- i >= quantile(i, prob=0.05, na.rm=TRUE) & i <= quantile(i, prob=0.95, na.rm=TRUE)
  return(res)
}


# cut down to reasonable size: using cLHS
f.subset <- function(i, n, non.id.vars) {
	# if there are more than n records, then sub-sample
	if(nrow(i) > n) {
	  # columns with IDs have been pre-filtered
		idx <- clhs(i[, non.id.vars], size=n, progress=FALSE, simple=TRUE, iter=1000)
		i.sub <- i[idx, ]
	}
	#	otherwise use what we have
	else
		i.sub <- i
	
	return(i.sub)
}


# set multi-row figure based on number of groups and fixed number of columns
dynamicPar <- function(n, max.cols=3) {
  # simplest case, fewer than max number of allowed columns
  if(n <= max.cols) {
    n.rows <- 1
    n.cols <- n
  } else {
    
    # simplest case, a square
    if(n %% max.cols == 0) {
      n.rows <- n / max.cols
      n.cols <- max.cols
    } else {
      # ragged
      n.rows <- round(n / max.cols) + 1
      n.cols <- max.cols
    }
  }
  
  par(mar=c(0,0,0,0), mfrow=c(n.rows, n.cols))
  # invisibly return geometry
  invisible(c(n.rows, n.cols))
}

# stat summary function
f.summary <- function(i, p) {
  
  # remove NA
  v <- na.omit(i$value)
  
  # compute quantiles
  q <- quantile(v, probs=p)
  res <- data.frame(t(q))
  
  ## TODO: implement better MADM processing and explanation  
  if(nrow(res) > 0) {
#     # MADM: MAD / median
#     # take the natural log of absolute values of MADM
#     res$log_abs_madm <- log(abs(mad(v) / median(v)))
#     # 0's become -Inf: convert to 0
#     res$log_abs_madm[which(is.infinite(res$log_abs_madm))] <- 0
    
    # assign reasonable names (quantiles)
    names(res) <- c(paste0('Q', p * 100))
    
    return(res)
  }
  else
    return(NULL)
}

# custom stats for box-whisker plot: 5th-25th-50th-75th-95th percentiles
# NOTE: we are re-purposing the coef argument!
# x: vector of values to summarize
# coef: Moran's I associated with the current raster
custom.bwplot <- function(x, coef=NA, do.out=FALSE) {
  # custom quantiles for bwplot
  stats <- quantile(x, p=c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = TRUE)
  # number of samples
  n <- length(na.omit(x))
  
  # compute effective sample size
  rho <- coef
  n_eff <- ESS_by_Moran_I(n, rho)
  
  # confidence "notch" is based on ESS
  iqr <- stats[4] - stats[2]
  conf <- stats[3] + c(-1.58, 1.58) * iqr/sqrt(n_eff)
  
  out.low <- x[which(x < stats[1])]
  out.high <- x[which(x > stats[5])]
  
  return(list(stats=stats, n=n, conf=conf, out=c(out.low, out.high)))
}

# load required packages
library(MASS, quietly=TRUE)
library(rgdal, quietly=TRUE)
library(rgeos, quietly=TRUE)
library(raster, quietly=TRUE)
library(plyr, quietly=TRUE)
library(reshape2, quietly=TRUE)
library(sharpshootR, quietly=TRUE)
library(latticeExtra, quietly=TRUE)
library(cluster, quietly=TRUE)
library(clhs, quietly=TRUE)
library(randomForest, quietly=TRUE)
library(spdep, quietly=TRUE)


## load local configuration
source('config.R')
```


```{r, echo=FALSE, results='hide'}
# load map unit polygons from OGR data source
mu <- try(readOGR(dsn=mu.dsn, layer=mu.layer, stringsAsFactors = FALSE))
if(class(mu) == 'try-error')
  stop(paste0('Cannot find map unit polygon file/feature: "', mu.dsn, ' / ', mu.layer, '"'), call. = FALSE)

# just in case, coerce mu.col to character
mu[[mu.col]] <- as.character(mu[[mu.col]])

# if no filter, then keep all map units
if(exists('mu.set')) {
  # coerce mu.set to character just in case
  mu.set <- as.character(mu.set)
  # filter
  mu <- mu[which(mu[[mu.col]] %in% mu.set), ]
} else {
  mu.set <- unique(mu[[mu.col]])
}

# nice colors
# 7 or fewer classes, use high-constrast colors
if(length(mu.set <= 7)) {
  cols <- brewer.pal(9, 'Set1') 
  # remove light colors
  cols <- cols[c(1:5,7,9)]
} else {
  # otherwise, use 12 paired colors
  cols <- brewer.pal(12, 'Paired')
}

# add a unique polygon ID
mu$pID <- seq(from=1, to=length(mu))

# check for cached data
if(cache.samples & file.exists('cached-samples.Rda')) {
  message('Using cached raster samples...')
  load('cached-samples.Rda')
} else {
  
  # iterate over map units and sample rasters
  # result is a list
  sampling.res <- sampleRasterStackByMU(mu, mu.set, mu.col, raster.list, pts.per.acre, estimateEffectiveSampleSize = correct.sample.size)

  # remove missing values: these will cause densityplot() to fail
  sampling.res$raster.samples <- na.omit(sampling.res$raster.samples)

  # cache for later
  if(cache.samples)
    save(sampling.res, file='cached-samples.Rda')
}


## find special variables and split
## gracefully handle missing rasters

# aspect
circ.vars <- names(raster.list)[grep('aspect', names(raster.list), ignore.case = TRUE)]
if(length(circ.vars) > 0) {
  do.aspect <- TRUE
  d.circ <- subset(sampling.res$raster.samples, subset=variable %in% circ.vars)
} else do.aspect <- FALSE
  
# geomorphons
geomorphons.vars <- names(raster.list)[grep('geomorphon', names(raster.list), ignore.case = TRUE)]
if(length(geomorphons.vars) > 0) {
  do.geomorphons <- TRUE
  d.geomorphons <- subset(sampling.res$raster.samples, subset=variable == geomorphons.vars)
} else do.geomorphons <- FALSE

# curvature classes
curvature.classes <- names(raster.list)[grep('curvature', names(raster.list), ignore.case = TRUE)]
if(length(curvature.classes) > 0) {
  do.curvature.classes <- TRUE
  d.curvature.classes <- subset(sampling.res$raster.samples, subset=variable == curvature.classes)
} else do.curvature.classes <- FALSE

# 2011 NLCD
nlcd.classes <- names(raster.list)[grep('nlcd', names(raster.list), ignore.case = TRUE)]
if(length(nlcd.classes) > 0) {
  do.nlcd.classes <- TRUE
  d.nlcd.classes <- subset(sampling.res$raster.samples, subset=variable == nlcd.classes)
} else do.curvature.classes <- FALSE


# everything else
sampling.res$raster.samples <- subset(sampling.res$raster.samples, subset=! variable %in% c(circ.vars, geomorphons.vars, curvature.classes))
```

<br>
<div style="text-align: center; border-top-style: solid; border-bottom-style: solid; border-top-width: 2px; border-bottom-width: 2px;"><span style="font-size: 200%; font-weight: bold;">Map units (`r mu.col`): `r paste(mu.set, collapse = ", ")`</span>
<br>
report version `r .report.version`
<br>
`r format(Sys.time(), "%Y-%m-%d")`</div>

<br>
This report is designed to provide statistical summaries of the environmental properties of one or more map units. Summaries are based on raster data extracted from fixed-density sampling (`r print(pts.per.acre)` samples / acre) of map unit polygons. Please see the document titled R-Based Map Unit Summary Report Introduction and Description for background and more information.



### Map Unit Polygon Data Source
```{r, echo=FALSE}
fd <- data.frame(`MU Polygons`=mu.dsn, `File or Feature`=mu.layer)
kable(fd, row.names = FALSE)
```

### Raster Data Sources
```{r, echo=FALSE}
kable(sampling.res$raster.summary, row.names = FALSE, digits = 3)
```

### Area Summaries
Consider increasing the sampling density (<b>`r pts.per.acre` points/ac.</b>) in `config.R` if there are unsampled polygons.
```{r, echo=FALSE}
kable(sampling.res$area.stats, caption='Map Unit Acreage by Polygon', align = 'r', col.names=c(mu.col, names(sampling.res$area.stats)[-1]))
```




### Modified Box and Whisker Plots
Whiskers extend from the 5th to 95th [percentiles](https://en.wikipedia.org/wiki/Percentile), the body represents the 25th through 75th percentiles, and the dot is the 50th percentile.

```{r, echo=FALSE, fig.width=8, fig.height=15}
tps <- list(box.rectangle=list(col='black'), box.umbrella=list(col='black', lty=1), box.dot=list(cex=0.5), plot.symbol=list(col=rgb(0.1, 0.1, 0.1, alpha = 0.25, maxColorValue = 1), cex=0.25))

# NOTE: notches rely on effective sampling size
bwplot(.id ~ value | variable, data=sampling.res$raster.samples, 
       scales=list(y=list(alternating=3), x=list(relation='free', tick.number=10)), as.table=TRUE, col='black', 
       strip=strip.custom(bg=grey(0.85)), xlab='', par.settings=tps, subscripts=TRUE, 
       layout=c(1, length(unique(sampling.res$raster.samples$variable))),
       panel=function(x, subscripts=subscripts, ...) {
         
         # extract the current raster name
         this.raster <- as.character(unique(sampling.res$raster.samples$variable[subscripts]))
         
         # get associated Moran's I
         idx <- which(sampling.res$Moran_I$raster.file == this.raster)
         this.Moran.I <- sampling.res$Moran_I$Moran.I[idx]
         
         # make a grid
         panel.grid(h=0, v=-1, col='grey', lty=3)
         panel.abline(h=1:length(unique(sampling.res$raster.samples$.id)), col='grey', lty=3)
         
         # boxplots with custom sampling size:
         # coef: Moran's I associated with this raster
         panel.bwplot(x, stats=custom.bwplot, notch=correct.sample.size, coef=this.Moran.I, ...)

       })


```


### Density Plots
These plots are a smooth alternative ([denisty estimation](https://en.wikipedia.org/wiki/Density_estimation)) to the classic "binned" ([histogram](https://en.wikipedia.org/wiki/Histogram)) approach to visualizing distributions. Peaks coorrospond to values that are most frequent within a dataset.


```{r, echo=FALSE, fig.width=8, fig.height=15}
tps <- list(superpose.line=list(col=cols, lwd=2, lend=2))

# dynamic setting of columns in legend
n.cols <- ifelse(length(mu.set) <= 4, length(mu.set), 5)

densityplot(~ value | variable, groups=.id, data=sampling.res$raster.samples, xlab='', ylab='', scales=list(relation='free', x=list(tick.number=10), y=list(at=NULL)), plot.points=FALSE, strip=strip.custom(bg=grey(0.85)), as.table=TRUE, layout=c(1, length(unique(sampling.res$raster.samples$variable))), auto.key=list(lines=TRUE, points=FALSE, columns=n.cols), par.settings=tps, type=c('l','g'))
```

### Tabular Summaries
Table of select [percentiles](https://en.wikipedia.org/wiki/Percentile), by variable.

```{r, echo=FALSE, results='asis'}
# summarize raster data for tabular output
mu.stats <- ddply(sampling.res$raster.samples, c('variable', '.id'), f.summary, p=p.quantiles)

# print medians
dg <- c(0, rep(2, times=length(unique(mu.stats$variable))))
mu.stats.wide <- dcast(mu.stats, .id ~ variable, value.var = 'Q50')
kable(mu.stats.wide, row.names=FALSE, caption = 'Median Values', align = 'r', digits=dg, col.names=c(mu.col, names(mu.stats.wide)[-1]))
```

```{r, echo=FALSE, results='asis'}
# iterate over variables and print smaller tables
# note: https://github.com/yihui/knitr/issues/886
l_ply(split(mu.stats, mu.stats$variable), function(i) {
  # remove variable column
  var.name <- unique(i$variable)
  i$variable <- NULL
  dg <- c(0, rep(2, times=length(p.quantiles)), 3)
  print(kable(i, caption = var.name, row.names=FALSE, align = 'r', digits=dg, col.names=c(mu.col, names(i)[-1])))
})

```


### Slope Aspect
A graphical summary of slope aspect values using density and percentile estimation methods adapated to circular data. Spread and central tendency are depicted with a combination of (circular) kernel density estimate and arrows. The 50th percentile value is shown with a red arrow and the 10th and 90th percentile values are shown with gray arrows. Arrow length is proportional to the strength of directionality.

```{r, echo=FALSE, results='hide', eval=do.aspect}
## circular stats, by map unit
d.circ.list <- split(d.circ, d.circ$.id)

# this has to be called 2x, as we are adjusting the device settings on the fly
fig.geom <- dynamicPar(length(d.circ.list))

# update default device output size
opts_chunk$set(fig.height=fig.geom[1] * 5) # rows
opts_chunk$set(fig.width=fig.geom[2] * 5) # cols
```


```{r, echo=FALSE, results='asis', eval=do.aspect}
# reset multi-figure plotting parameters
dynamicPar(length(d.circ.list))

res <- ldply(d.circ.list, function(i) {
  mu <- unique(i$.id)
  circ.stats <- aspect.plot(i$value, q=c(0.1, 0.5, 0.9), plot.title=mu, pch=NA, bg='RoyalBlue', col='black', arrow.col=c('grey', 'red', 'grey'), stack=FALSE, p.bw=90)
  
  return(round(circ.stats))
})

# tabular summary
kable(res, align = 'r', col.names=c(mu.col, names(res)[-1]))
```


### Slope Shape (Curvature) Summary
The classes were generated using a 5x5 moving window, from a regional 30m DEM. The precision may be limited, use with caution. See instructions for using your own (higher resolution) curvature classification raster.
```{r, echo=FALSE, fig.width=12, fig.height=6, eval=do.curvature.classes}
# set names: from Field Guide for description of soils
## source data: opposite convention
# 1's place: profile curvature
# 10's place: plan curvature
#
## adapted from above
## data are reported down/across slope
# L/L | L/V | L/C         22 | 32 | 12  
# V/L | V/V | V/C   ----> 23 | 33 | 13
# C/L | C/V | C/C         21 | 31 | 11
#
# order according to approximate "shedding"" -> "accumulating" gradient:
# 'V/V', 'L/V', 'V/L', 'C/V', 'LL', 'C/L', 'V/C', 'L/C', 'C/C'
#
d.curvature.classes$value <- factor(d.curvature.classes$value, 
                                    levels=c(33, 32, 23, 31, 22, 21, 13, 12, 11), 
                                    labels = c('V/V', 'L/V', 'V/L', 'C/V', 'LL', 'C/L', 'V/C', 'L/C', 'C/C'))

# tabulate and convert to proportions
x <- xtabs(~ .id + value, data=d.curvature.classes)
x <- round(sweep(x, MARGIN = 1, STATS = rowSums(x), FUN = '/'), 3)

# print
kable(x)

# convert to long format for plotting
x.long <- melt(x)
# fix names: second column contains curvature class labels
names(x.long)[2] <- 'curvature.class'

# make some colors, and set style
cols.curvature.classes <- brewer.pal(9, 'Spectral')
tps <- list(superpose.polygon=list(col=cols.curvature.classes, lwd=2, lend=2))

# no re-ordering of musym
trellis.par.set(tps)
barchart(as.character(.id) ~ value, groups=curvature.class, data=x.long, horiz=TRUE, stack=TRUE, xlab='Proportion of Samples', scales=list(cex=1.5), key=simpleKey(space='top', columns=3, text=levels(x.long$curvature.class), rectangles = TRUE, points=FALSE))
```




### Geomorphon Landform Classification
Proportion of samples within each map unit that corrospond to 1 of 10 possible landform positions, as generated via [geomorphon](https://grass.osgeo.org/grass70/manuals/addons/r.geomorphon.html) algorithm.

```{r, echo=FALSE, eval=do.geomorphons, fig.width=12, fig.height=6}
## geomorphons:
# set names
# https://grass.osgeo.org/grass70/manuals/addons/r.geomorphon.html
d.geomorphons$value <- factor(d.geomorphons$value, levels=1:10, labels = c('flat', 'summit', 'ridge', 'shoulder', 'spur', 'slope', 'hollow', 'footslope', 'valley', 'depression'))

# tabulate and convert to proportions
x <- xtabs(~ .id + value, data=d.geomorphons)
x <- sweep(x, MARGIN = 1, STATS = rowSums(x), FUN = '/')

# create a signature of the most frequent classes that sum to 75% or
x.sig <- apply(x, 1, function(i) {
  # order the proportions of each row
  o <- order(i, decreasing = TRUE)
  # determine a cut point for cumulative proportion >= threshold value
  thresh.n <- which(cumsum(i[o]) >= 0.75)[1]
  # if there is only a single class that dominates, then offset index as we subtract 1 next
  if(thresh.n == 1)
    thresh.n <- 2
  # get the top classes
  top.classes <- i[o][1:(thresh.n-1)]
  # format for adding to a table
  paste(names(top.classes), collapse = '/')
}
)
# prepare for printing HTML table
x.sig <- as.data.frame(x.sig)
names(x.sig) <- 'signature'

# get a geomorphon signature for each polygon
geomorphon.spatial.summary <- ddply(d.geomorphons, c('pID', '.id'), function(i) {
  # tabulate and convert to proportions
  x <- xtabs(~ .id + value, data=i)
  x <- sweep(x, MARGIN = 1, STATS = rowSums(x), FUN = '/')
  return(x)
})

## consider supervised classification for QC at this stage:
# r <- randomForest(factor(.id) ~ ., data=geomorphon.spatial.summary[, -1])
# geomorphon.spatial.summary$pred.id <- predict(r, geomorphon.spatial.summary)

## most likely landform
most.likely.landform.idx <- apply(geomorphon.spatial.summary[, -c(1:2)], 1, which.max)
geomorphon.spatial.summary$ml_landform <- levels(d.geomorphons$value)[most.likely.landform.idx]

## shannon H by polygon
geomorphon.spatial.summary$shannon_h <- apply(geomorphon.spatial.summary[, 3:10], 1, function(i) {
  -sum(i * log(i, base=10), na.rm=TRUE)
})

## TODO: is this of any use?
# bwplot(.id ~ shannon_h, data=geomorphon.spatial.summary, xlab='Shannon Entropy')


## convert proportions to long format for plotting
x.long <- melt(x)
# fix names: second column contains geomorphon labels
names(x.long)[2] <- 'geomorphon'

# make some colors, and set style
cols.geomorphons <- c('grey', brewer.pal(9, 'Spectral'))
tps <- list(superpose.polygon=list(col=cols.geomorphons, lwd=2, lend=2))

# clustering of proportions only works with >1 group
if(length(unique(x.long$.id)) > 1) {
  # cluster proportions
  x.d <- as.hclust(diana(daisy(x)))
  # re-order MU labels levels based on clustering
  x.long$.id <- factor(x.long$.id, levels=x.long$.id[x.d$order])
  
  # musym are re-ordered according to clustering
  trellis.par.set(tps)
  barchart(.id ~ value, groups=geomorphon, data=x.long, horiz=TRUE, stack=TRUE, xlab='Proportion of Samples', scales=list(cex=1.5), key=simpleKey(space='top', columns=5, text=levels(x.long$geomorphon), rectangles = TRUE, points=FALSE), legend=list(right=list(fun=dendrogramGrob, args=list(x = as.dendrogram(x.d), side="right", size=10))))
} else {
  # re-order MU labels levels based on clustering
  x.long$.id <- factor(x.long$.id)
  
  trellis.par.set(tps)
  barchart(.id ~ value, groups=geomorphon, data=x.long, horiz=TRUE, stack=TRUE, xlab='Proportion of Samples', scales=list(cex=1.5), key=simpleKey(space='top', columns=5, text=levels(x.long$geomorphon), rectangles = TRUE, points=FALSE))
}
```

```{r, echo=FALSE, eval=do.geomorphons}
# print and truncate to 2 decimal places
kable(x, digits = 2, caption = '')
```


```{r, echo=FALSE, eval=do.geomorphons}
kable(x.sig, caption = 'Top 75% fraction of landform classes in decreasing order.')
```


### Landcover Summary (2011 NLCD)

```{r, echo=FALSE, fig.width=12, fig.height=10, eval=do.nlcd.classes}
# These are from the NLCD 2011 metadata
nlcd.leg <- structure(list(ID = c(0L, 11L, 12L, 21L, 22L, 23L, 24L, 31L, 
41L, 42L, 43L, 51L, 52L, 71L, 72L, 73L, 74L, 81L, 82L, 90L, 95L
), name = c("nodata", "Open Water", "Perennial Ice/Snow", "Developed, Open Space", 
"Developed, Low Intensity", "Developed, Medium Intensity", "Developed, High Intensity", 
"Barren Land (Rock/Sand/Clay)", "Deciduous Forest", "Evergreen Forest", 
"Mixed Forest", "Dwarf Scrub", "Shrub/Scrub", "Grassland/Herbaceous", 
"Sedge/Herbaceous", "Lichens", "Moss", "Pasture/Hay", "Cultivated Crops", 
"Woody Wetlands", "Emergent Herbaceous Wetlands"), col = c("#000000", 
"#476BA0", "#D1DDF9", "#DDC9C9", "#D89382", "#ED0000", "#AA0000", 
"#B2ADA3", "#68AA63", "#1C6330", "#B5C98E", "#A58C30", "#CCBA7C", 
"#E2E2C1", "#C9C977", "#99C147", "#77AD93", "#DBD83D", "#AA7028", 
"#BAD8EA", "#70A3BA")), .Names = c("ID", "name", "col"), row.names = c(NA, 
-21L), class = "data.frame")

# set factor levels
d.nlcd.classes$value <- factor(d.nlcd.classes$value, 
                                    levels = nlcd.leg$ID, 
                                    labels = nlcd.leg$name)

# tabulate and convert to proportions
x <- xtabs(~ .id + value, data=d.nlcd.classes)
x <- round(sweep(x, MARGIN = 1, STATS = rowSums(x), FUN = '/'), 3)

# remove 0's
idx <- which(apply(x, 2, function(i) all(i != 0)))
x <- x[, idx]

# print
kable(x)

# convert to long format for plotting
x.long <- melt(x)
# fix names: second column contains NLCD class labels
names(x.long)[2] <- 'nlcd.class'


# These are from the NLCD 2011 metadata
# get colors for only those classes in this data
cols.nlcd.classes <- nlcd.leg$col[match(levels(x.long$nlcd.class), nlcd.leg$name)]
tps <- list(superpose.polygon=list(col=cols.nlcd.classes, lwd=2, lend=2))

# no re-ordering of musym
trellis.par.set(tps)
barchart(as.character(.id) ~ value, groups=nlcd.class, data=x.long, horiz=TRUE, stack=TRUE, xlab='Proportion of Samples', scales=list(cex=1.5), key=simpleKey(space='top', columns=3, text=levels(x.long$nlcd.class), rectangles = TRUE, points=FALSE))
```



### Multivariate Summary

This plot displays the similarity of the map units across the set of environmental variables used in this report. The contours contain 50% of the points (sub-sampled via [cLHS](https://en.wikipedia.org/wiki/Latin_hypercube_sampling)) in an optimal [2D projection](https://en.wikipedia.org/wiki/Multidimensional_scaling#Non-metric_multidimensional_scaling) of multivariate data space. Map units with very low variation in environmental variables can result in tightly clustered points in the 2D projection. In these cases it may not be possible to generate a 50%-data contour. It is not possible to generate a multivariate summary when any sampled variable (e.g. slope) has a near-zero variance.


```{r, results='hide', echo=FALSE, fig.width=8, fig.height=8}
## TODO: 
# 1. add geomorphons, curvature classses, and possibly NLCD (factors)

# cast to wide format
d.mu.wide <- dcast(sampling.res$raster.samples, sid + pID + .id ~ variable, value.var = 'value')

# locate "non-id" vars
d.mu.wide.vars <- which(! names(d.mu.wide) %in% c('.id', 'sid', 'pID'))

# check SD of each column, by group
sd.by.id <- ddply(d.mu.wide, '.id', function(i) {sapply(i[, d.mu.wide.vars], sd, na.rm=TRUE)})
sd.by.id$res <- apply(sd.by.id[, -1], 1, function(i) any(i < 1e-5))


# if the SD is low in any column from all MU then stop
if(all(sd.by.id$res)) {
  multivariate.summary <- FALSE
} else {
  # OK to run MV summaries
  multivariate.summary <- TRUE
  
  # filter out low-variance MU
  if(any(sd.by.id$res)) {
    ids.to.keep <- sd.by.id$.id[which(!sd.by.id$res)]
    d.mu.wide <- d.mu.wide[which(d.mu.wide$.id %in% ids.to.keep), ]
    
    # reset mu.set accordingly
    idx.to.keep <- which(! mu.set %in% setdiff(mu.set, ids.to.keep))
    mu.set <- mu.set[idx.to.keep]
  }
  
  ## TODO: what is a reasonable sample size?
  # only sub-sample if there are "a lot" of samples
  if(nrow(d.mu.wide) > 1000) {
    # sub-sample via LHS: this takes time
    # first three columns are IDs
    # n: this is the number of sub-samples / map unit
    # non.id.vars: this is an index to non-ID columns
    d.sub <- ddply(d.mu.wide, '.id', f.subset, n=50, non.id.vars=d.mu.wide.vars)
  } else {
    d.sub <- d.mu.wide
  }
  
  ## NOTE: data with very low variability will cause warnings
  # eval numerical distance, removing 'sid' and '.id' columns
  d.dist <- daisy(d.sub[, d.mu.wide.vars], stand=TRUE)
  
  ## convert .id to factor for correct color matching
  d.sub$.id <- factor(d.sub$.id, levels=mu.set)
  
  ## map distance matrix to 2D space via principal coordinates
  d.betadisper <- vegan::betadisper(d.dist, group=d.sub$.id, bias.adjust = TRUE, sqrt.dist = TRUE, type='median')
  d.scores <- vegan::scores(d.betadisper)
  
  ## TODO: there might be a better way to do this, ask Jay
  # contour density estimates
  # add contours for fixed pct of data density using KDE
  # other ideas: https://stat.ethz.ch/pipermail/r-help/2012-March/305425.html
  s <- data.frame(x=d.scores$sites[, 1], y=d.scores$sites[, 2], .id=d.sub$.id)
  s <- split(s, s$.id)
  
  
  # default plot is OK, but density-based contours are more useful
  # vegan:::plot.betadisper(d.betadisper, ellipse = TRUE, hull = FALSE, col=cols[1:length(mu.set)], conf=0.5, segments=FALSE, xlab='', ylab='', main='', sub='', las=1)
  
  # plot
  par(mar=c(1,1,3,1))
  plot(d.scores$sites, type='n', axes=FALSE)
  abline(h=0, v=0, lty=2, col='grey')
  
  # NOTE: lines are not added if data are too densely spaced for evaluation of requested prob. level 
  # add contours of prob density
  res <- lapply(s, kdeContours, prob=c(0.5))
  
  points(d.scores$sites, cex=0.45, col=cols[match(d.sub$.id, mu.set)], pch=16)
  vegan::ordilabel(d.betadisper, display='centroids', col=cols)
  title('Ordination of Raster Samples (cLHS Subset) with 50% Density Contour')
  box()
  # legend('topleft', legend=mu.set, lwd=NA, pch=16, col=cols[1:length(mu.set)], bty='n', cex=1.25)
}
```

```{r, echo=FALSE}
if(multivariate.summary == FALSE)
  print('Cannot create ordination plot: not enough variance within each map unit.')
```


### Raster Data Correlation
The following figure highlights shared information among raster data sources based on [Spearman's Ranked Correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient). Branch height is associated with the degree of shared information between raster data.
```{r, echo=FALSE, fig.width=10, fig.height=8, eval=multivariate.summary}
par(mar=c(2,5,2,2))
## note that we don't load the Hmisc package as it causes many NAMESPACE conflicts
## This requires 3 or more variables
if(ncol(d.sub[, d.mu.wide.vars]) > 3) {
  try(plot(Hmisc::varclus(as.matrix(d.sub[, d.mu.wide.vars]))), silent=TRUE)
} else
  print('This plot requires three or more raster variables, apart from aspect, curvature class, and geomorphons.')
```


### Raster Data Importance

```{r, echo=FALSE, fig.width=8, fig.height=6, eval=multivariate.summary}
# this will only work with >= 2 map units
if(length(table(d.sub$.id)) >= 2) {
 # use supervised classification to empirically determine the relative importance of each raster layer
  # TODO: include geomorphons and curvature classes
  # TODO: consider using party::cforest() for conditional variable importance-- varimp
  m <- randomForest(x=d.sub[, d.mu.wide.vars], y=d.sub$.id, importance = TRUE)
  
  # variable importance
  # TODO: how to interpret raw output from importance:
  # http://stats.stackexchange.com/questions/164569/interpreting-output-of-importance-of-a-random-forest-object-in-r/164585#164585
  varImpPlot(m, scale=FALSE, type=2, main='')
  # kable(importance(m, scale=FALSE, type=2), digits = 3)
  
  # ## this adds several seconds to processing time
  # # predict using samples from each polygon, to get proportions of each MU
  # d.mu.wide <- na.omit(d.mu.wide)
  # d.mu.wide$.predMU <- as.character(predict(m, d.mu.wide))
  # 
  # ## TODO: add number of samples / sid
  # # compute proportion of each class by sid
  # pred.by.sid <- ddply(d.mu.wide, 'sid', .fun=function(i) {
  #   # explicit setting of levels results in exact output each iteration
  #   prop.table(table(factor(i$.predMU, levels=mu.set)))
  # })
  
  ## TODO
  # compute Shannon Entropy at each sid and fix names (useful?)
  
  ## TODO: join with output SHP via sid 
} else {
  # print message about not enough map unit s
  print('This plot requires two or more map units.')
}



```


### Flagged Polygons
These polygons, identified by `pID` in the output data, are associated with 15% or more samples that are outside of the 5th-95th percentile range. Flagged polygons are ranked in the output SHP file.
```{r, echo=FALSE}
# apply across raster values, to all polygons
polygons.to.check <- ddply(sampling.res$raster.samples, c('.id', 'variable'), flagPolygons)

# fix names for printing
names(polygons.to.check)[1] <- mu.col

# print table
kable(polygons.to.check, row.names = FALSE)
```




### Save Results Locally
Results are saved in a folder called "output" in the working directory.
```{r echo=FALSE}
# make an output dir if it doesn't exist
if(!dir.exists('output')) dir.create('./output')

# save SHP with any un-sampled polygons
if(length(sampling.res$unsampled.ids) > 0) {
  shp.fname <- paste0('un-sampled-', paste(mu.set, collapse='_'))
  writeOGR(mu[sampling.res$unsampled.ids, ], dsn='output', layer=shp.fname, driver='ESRI Shapefile', overwrite_layer=TRUE)
}

# compute summaries
poly.stats <- ddply(sampling.res$raster.samples, c('pID', 'variable'), f.summary, p=p.quantiles)

# convert to wide format, keeping median value
poly.stats.wide <- dcast(poly.stats, pID ~ variable, value.var = 'Q50')
# # convert to wide format, keeping log_abs_madm
# poly.stats.wide.2 <- dcast(poly.stats, pID ~ variable, value.var = 'log_abs_madm')

# add a suffix to variable names so that we can combine
# names(poly.stats.wide.1)[-1] <- paste0(names(poly.stats.wide.1)[-1], '_med')
# names(poly.stats.wide.2)[-1] <- paste0(names(poly.stats.wide.2)[-1], '_var')

## TODO: pending further review
# join median + MADM stats for each polygon
# poly.stats.wide <- join(poly.stats.wide.1, poly.stats.wide.2, by='pID')
# poly.stats.wide <- poly.stats.wide.1


# save
poly.stats.fname <- paste0('output/poly-stats-', paste(mu.set, collapse='_'), '.csv')
write.csv(poly.stats.wide, file=poly.stats.fname, row.names=FALSE)

## prep variable names for SHP column names
# cannot contain reserved characters
# 10 char limit
names(poly.stats.wide)[-1] <- sapply(names(poly.stats.wide[, -1]), function(i) {
  # remove '.' and get prefix
  prfx <- gsub('.', '', substr(i, 1, nchar(i)-4), fixed=TRUE)
  # abbreviate after filtering other bad chars
  abbr <- abbreviate(gsub('%|/|\\(|\\)', '', prfx), minlength = 6)
  # extract suffix
  suffix <- substr(i, nchar(i)-3, nchar(i))
  # re-combine
  res <- paste0(abbr, suffix)
  return(res)
  })

## join stats to map unit polygon attribute table
mu <- merge(mu, poly.stats.wide, by='pID', all.x=TRUE)

## join geomorphon class stats to attribute table
if(do.geomorphons)
  mu <- merge(mu, geomorphon.spatial.summary, by='pID', all.x=TRUE)

## flag polygons with n% outside of 5-95 pctiles
mu[['toCheck']] <- 0

# score and rank flagged data
# sum of proportion outside of 5-95 range, over each variable
ranked.polygons.to.check <- rank(tapply(polygons.to.check$prop.outside.range, polygons.to.check$pID, sum))
# sort out which pIDs are flagged
flagged.pIDs <- names(ranked.polygons.to.check)
flagged.idx <- which(mu$pID %in% flagged.pIDs)
# save ranked flags
mu$toCheck[flagged.idx] <- ranked.polygons.to.check[match(mu$pID[flagged.idx], names(ranked.polygons.to.check))]


# save to file
shp.fname <- paste0('polygons-with-stats-', paste(mu.set, collapse='_'))
writeOGR(mu, dsn='output', layer=shp.fname, driver='ESRI Shapefile', overwrite_layer=TRUE)


## TODO: how do you trap warnings within a .Rmd knitting session?
# save warnings to log file
# cat(warnings(), file = 'output/warning-log.txt')
```




----------------------------
This document is based on `sharpshootR` version `r utils::packageDescription("sharpshootR", field="Version")`.
<br>
Report [configuration and source code are hosted on GitHub](https://github.com/ncss-tech/soilReports).

